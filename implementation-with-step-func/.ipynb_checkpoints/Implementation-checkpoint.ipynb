{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation with Step Functions\n",
    "\n",
    "Now that we have a satisfying machine learning model, we want to implement this as part of a process that runs every day at 3AM in the morning based on the latest transactional information that we have dumped on S3 to create forecasts for each reseller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload data to S3\n",
    "\n",
    "First you need to create a bucket for this experiment. Upload the data from the following public location to your own S3 bucket. To facilitate the work of the crawler use two different prefixs (folders): one for the billing information and one for reseller. \n",
    "\n",
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your bucket name\n",
    "your_bucket = 'blackb-mggaska-implementation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-02 16:35:59--  https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/billing_sm.csv\n",
      "Resolving ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)... 52.216.20.203\n",
      "Connecting to ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)|52.216.20.203|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15803443 (15M) [binary/octet-stream]\n",
      "Saving to: ‘billing_sm.csv.1’\n",
      "\n",
      "billing_sm.csv.1    100%[===================>]  15.07M  99.2MB/s    in 0.2s    \n",
      "\n",
      "2019-07-02 16:35:59 (99.2 MB/s) - ‘billing_sm.csv.1’ saved [15803443/15803443]\n",
      "\n",
      "--2019-07-02 16:35:59--  https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/reseller_sm.csv\n",
      "Resolving ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)... 52.216.20.203\n",
      "Connecting to ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)|52.216.20.203|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 210111 (205K) [binary/octet-stream]\n",
      "Saving to: ‘reseller_sm.csv.1’\n",
      "\n",
      "reseller_sm.csv.1   100%[===================>] 205.19K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2019-07-02 16:35:59 (32.0 MB/s) - ‘reseller_sm.csv.1’ saved [210111/210111]\n",
      "\n",
      "--2019-07-02 16:35:59--  https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/awswrangler-0.0b2-py3.6.egg\n",
      "Resolving ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)... 52.216.20.203\n",
      "Connecting to ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)|52.216.20.203|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33511 (33K) [binary/octet-stream]\n",
      "Saving to: ‘awswrangler-0.0b2-py3.6.egg’\n",
      "\n",
      "awswrangler-0.0b2-p 100%[===================>]  32.73K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2019-07-02 16:35:59 (49.1 MB/s) - ‘awswrangler-0.0b2-py3.6.egg’ saved [33511/33511]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/billing_sm.csv\n",
    "!wget https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/reseller_sm.csv\n",
    "!wget https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/awswrangler-0.0b2-py3.6.egg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we upload to an S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, os\n",
    "boto3.Session().resource('s3').Bucket(your_bucket).Object(os.path.join('billing', 'billing_sm.csv')).upload_file('billing_sm.csv')\n",
    "boto3.Session().resource('s3').Bucket(your_bucket).Object(os.path.join('reseller', 'reseller_sm.csv')).upload_file('reseller_sm.csv')\n",
    "boto3.Session().resource('s3').Bucket(your_bucket).Object(os.path.join('python', 'awswrangler-0.0b2-py3.6.egg')).upload_file('awswrangler-0.0b2-py3.6.egg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Crawler\n",
    "\n",
    "To use this csv information in the context of a Glue ETL, first we have to create a Glue crawler pointing to the location of each file. The crawler will try to figure out the data types of each column. The safest way to do this process is to create one crawler for each table pointing to a different location.\n",
    "\n",
    "1. Go to the AWS Console.\n",
    "2. Select under Services AWS Glue.\n",
    "3. Under crawlers Add Crawler and two crawlers: create one pointing to each S3 location (one to billing and one to reseller)\n",
    "\n",
    "    3.1 Name: Billing, Data Store, Specific Path in my Account, Navigate to your bucket and your folder Billing, create an IAM role billing-crawler-role, add database implementationdb, Next, Finish\n",
    "    \n",
    "    3.2 After the crawler is created select Run it now.\n",
    "    \n",
    "    3.3 Name: Reseller, Data Store, Specific Path in my Account, Navigate to your bucket and your folder Reseller, create an IAM role reseller-crawler-role, select database implementationdb, Next, Finish\n",
    "    \n",
    "    3.4 After the crawler is created select Run it now.\n",
    "\n",
    "After both crawlers run you should see one table is been adeed for each. You can use Athena to inspect the tables and double check the data is been added properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Glue Job\n",
    "\n",
    "Now we are going to create a GLUE ETL job in python 3.6. In this job we can combine both the ETL from Notebook #2 and the Preprocessing Pipeline from Notebook #4.\n",
    "\n",
    "Note that instead of reading from a csv file we are going to use Athena to read from the resulting tables of the Glue Crawler. \n",
    "\n",
    "Glue is a \"serverless\" service so the processing power assigned to the process is meassured in DPUs. Each DPU is equivalent to 16GB of RAM and 4vCPU. \n",
    "\n",
    "1. Open the AWS Console\n",
    "2. Under Services go to AWS Glue\n",
    "3. Uner Jobs, add new job\n",
    "4. Name: etlandpipeline, Role: Glueadmin, Type Python Shell, Python3, Select A New Script Authored By you,\n",
    "Under Security Configuration...  Select Python library path and browse to the location where you have the egg of the aws wrangler Library, Under Maximum Capacity write 1. Then hit \"Save Job and Edit Script\"\n",
    "\n",
    "In the Script tab copy and paste the following script adapted to Glue from the previous notebooks. \n",
    "<b> ALWAYS REMEMBER TO ADAPT YOUR ROLE ARN AND BUCKET </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\r\n",
      "import numpy as np\r\n",
      "import datetime \r\n",
      "import pandas as pd\r\n",
      "from datetime import date\r\n",
      "import numpy as np\r\n",
      "from sklearn.preprocessing import OneHotEncoder\r\n",
      "from sklearn.preprocessing import LabelEncoder\r\n",
      "import boto3\r\n",
      "import pickle\r\n",
      "import io\r\n",
      "from io import StringIO\r\n",
      "import awswrangler\r\n",
      "\r\n",
      "\r\n",
      "df_r = awswrangler.athena.read( \"implementationdb\", \"select * from reseller\" )\r\n",
      "df = awswrangler.athena.read( \"implementationdb\", \"select * from billing\" )\r\n",
      "bucket = 'blackb-mggaska-implementation'\r\n",
      "df['date'] = pd.to_datetime(df['date'])\r\n",
      "\r\n",
      "\r\n",
      "print('dataframe',df.shape)\r\n",
      "print('dataframer',df_r.shape)\r\n",
      "\r\n",
      "#---FUNCTIONS-------------------------------\r\n",
      "\r\n",
      "def write_dataframe_to_csv_on_s3(dataframe, bucket, filename):\r\n",
      "    \"\"\" Write a dataframe to a CSV on S3 \"\"\"\r\n",
      "    # Create buffer\r\n",
      "    csv_buffer = StringIO()\r\n",
      "    # Write dataframe to buffer\r\n",
      "    dataframe.to_csv(csv_buffer, sep=\",\", header=None,index=None)\r\n",
      "    # Create S3 object\r\n",
      "    s3_resource = boto3.resource(\"s3\") \r\n",
      "    # Write buffer to S3 object\r\n",
      "    s3_resource.Object(bucket, filename).put(Body=csv_buffer.getvalue())\r\n",
      "    print(\"Writing {} records to {}\".format(len(dataframe), filename))\r\n",
      "    \r\n",
      "#--------------------------------------------------\r\n",
      "# ### Filter the last 4 months of data\r\n",
      "\r\n",
      "today = date.today()\r\n",
      "min_date = today - pd.to_timedelta(120, unit='d')\r\n",
      "\r\n",
      "df = df[df['date'] > min_date]\r\n",
      "\r\n",
      "def completeItem(dfItem):\r\n",
      "    min_date = dfItem['date'].min()\r\n",
      "    max_date = dfItem['date'].max()\r\n",
      "    if min_date == max_date:\r\n",
      "        #only one data point\r\n",
      "        return\r\n",
      "    r = pd.date_range(start=min_date, end=max_date)\r\n",
      "    dfItemNew = dfItem.set_index('date').reindex(r).rename_axis('date').reset_index()\r\n",
      "    \r\n",
      "    dfItemNew['mean-last-30'] = dfItemNew['bill'].rolling(30,min_periods=1).mean().reset_index()['bill']\r\n",
      "    dfItemNew['mean-last-7'] = dfItemNew['bill'].rolling(7,min_periods=1).mean().reset_index()['bill']\r\n",
      "    dfItemNew['std-last-30'] = dfItemNew['bill'].rolling(30,min_periods=1).std().reset_index()['bill']\r\n",
      "    dfItemNew['bill'] = dfItemNew['bill'].fillna(0)\r\n",
      "    dfItemNew['id_reseller'] = dfItem['id_reseller'].max()\r\n",
      "    dfItemNew['std-last-30'].fillna(method='ffill',inplace=True)\r\n",
      "    dfItemNew['mean-last-7'].fillna(method='ffill',inplace=True)\r\n",
      "    dfItemNew['std-last-30'].fillna(method='ffill',inplace=True)\r\n",
      "    resp = []\r\n",
      "    counter = 0\r\n",
      "    for index,row in dfItemNew.iterrows(): \r\n",
      "        resp.append(counter)\r\n",
      "        if row['bill'] == 0: \r\n",
      "            counter += 1 \r\n",
      "        else:\r\n",
      "            counter = 0\r\n",
      "    dfItemNew['days_without_purchase'] = pd.Series(resp)\r\n",
      "    return dfItemNew\r\n",
      "\r\n",
      "i = 0\r\n",
      "dfCompletedList = []\r\n",
      "for nid,item in df.groupby('id_reseller'):\r\n",
      "    i = i+1\r\n",
      "    if i%200 == 0:\r\n",
      "        print ('processed {} resellers'.format(str(i)))\r\n",
      "    dfCompletedList.append(completeItem(item))\r\n",
      "\r\n",
      "\r\n",
      "df = pd.concat(dfCompletedList).copy()\r\n",
      "del dfCompletedList\r\n",
      "df['weekday']  = df['date'].dt.weekday_name\r\n",
      "\r\n",
      "\r\n",
      "# ### Compute next bill\r\n",
      "\r\n",
      "# In[11]:\r\n",
      "\r\n",
      "\r\n",
      "df['next_bill'] = df.replace(0,np.nan).groupby('id_reseller')['bill'].fillna(method='bfill')\r\n",
      "\r\n",
      "\r\n",
      "# ## Compute last bill\r\n",
      "\r\n",
      "# In[12]:\r\n",
      "\r\n",
      "\r\n",
      "df['last_bill'] = df.replace(0,np.nan).groupby('id_reseller')['bill'].fillna(method='ffill').copy()\r\n",
      "different_zero = df['last_bill'].shift(1)\r\n",
      "df.loc[df['bill'] != 0,'last_bill'] = np.nan\r\n",
      "df['last_bill'] = df['last_bill'].fillna(different_zero)\r\n",
      "\r\n",
      "\r\n",
      "# In[13]:\r\n",
      "\r\n",
      "\r\n",
      "df = df.merge(df_r,how='inner',on='id_reseller')\r\n",
      "\r\n",
      "\r\n",
      "# In[14]:\r\n",
      "\r\n",
      "\r\n",
      "df = df.dropna()\r\n",
      "\r\n",
      "\r\n",
      "# ## Deal with categorical variables\r\n",
      "# \r\n",
      "# To deal with categorical variables (reseller's cluster and reseller's zone), we will use a combination of sklearn's Label Encoder, a preprocessing module that transforms strings in numeric lables, and One Hot Encoder, that takes this numerical variables and creates dummy (0/1 state) variables. \r\n",
      "# \r\n",
      "# This modules are python objects that keep in their internal variables the information necessary to transform new data.  So, in the Glue ETL we are going to store this objects in pkl format\r\n",
      "# \r\n",
      "\r\n",
      "# In[17]:\r\n",
      "\r\n",
      "\r\n",
      "le_cluster = LabelEncoder()\r\n",
      "ohe_cluster = OneHotEncoder(handle_unknown='ignore')\r\n",
      "df_cluster = pd.DataFrame(ohe_cluster.fit_transform(le_cluster.fit_transform(df['cluster'].fillna('')).reshape(-1, 1)).todense())\r\n",
      "df_cluster = df_cluster.add_prefix('cluster_')\r\n",
      "\r\n",
      "\r\n",
      "# In[18]:\r\n",
      "\r\n",
      "\r\n",
      "le_zone = LabelEncoder()\r\n",
      "ohe_zone = OneHotEncoder(handle_unknown='ignore')\r\n",
      "df_zone = pd.DataFrame(ohe_zone.fit_transform(le_zone.fit_transform(df['zone'].fillna('')).reshape(-1, 1)).todense())\r\n",
      "df_zone = df_zone.add_prefix('zone_')\r\n",
      "\r\n",
      "\r\n",
      "# In[19]:\r\n",
      "\r\n",
      "\r\n",
      "le_weekday = LabelEncoder()\r\n",
      "ohe_weekday = OneHotEncoder(handle_unknown='ignore')\r\n",
      "df_weekday = pd.DataFrame(ohe_weekday.fit_transform(le_weekday.fit_transform(df['weekday']).reshape(-1, 1)).todense())\r\n",
      "df_weekday = df_weekday.add_prefix('weekday_')\r\n",
      "\r\n",
      "\r\n",
      "# In[20]:\r\n",
      "\r\n",
      "\r\n",
      "client = boto3.client('s3')\r\n",
      "client.put_object(Body=pickle.dumps(le_cluster), Bucket=bucket, Key='preprocessing/le_cluster.pkl');\r\n",
      "\r\n",
      "\r\n",
      "# In[21]:\r\n",
      "\r\n",
      "\r\n",
      "client.put_object(Body=pickle.dumps(ohe_cluster), Bucket=bucket, Key='preprocessing/ohe_cluster.pkl')\r\n",
      "client.put_object(Body=pickle.dumps(le_zone), Bucket=bucket, Key='preprocessing/le_zone.pkl')\r\n",
      "client.put_object(Body=pickle.dumps(ohe_zone), Bucket=bucket, Key='preprocessing/ohe_zone.pkl')\r\n",
      "client.put_object(Body=pickle.dumps(le_weekday), Bucket=bucket, Key='preprocessing/le_weekday.pkl')\r\n",
      "client.put_object(Body=pickle.dumps(ohe_weekday), Bucket=bucket, Key='preprocessing/ohe_weekday.pkl');\r\n",
      "\r\n",
      "\r\n",
      "# ## Write to S3 resulting ETL\r\n",
      "# \r\n",
      "# Now we have to write to S3 all the relevant columns. We will perform a train/validation split of the customers so we can train on a group and get relevant metrics on the other.\r\n",
      "\r\n",
      "# In[29]:\r\n",
      "\r\n",
      "\r\n",
      "df = df[['next_bill', 'bill', 'date', 'id_reseller', 'mean-last-30', 'mean-last-7',\r\n",
      "       'std-last-30', 'days_without_purchase', 'weekday', \r\n",
      "       'last_bill', 'zone', 'cluster']]\r\n",
      "\r\n",
      "df = pd.concat([df,df_cluster,df_zone,df_weekday],axis=1)\r\n",
      "\r\n",
      "\r\n",
      "#Take a random 10% sample of the resellers\r\n",
      "val_resellers = list(pd.Series(df['id_reseller'].unique()).sample(frac=0.1))\r\n",
      "\r\n",
      "df_train = df[~df['id_reseller'].isin(val_resellers)].sample(frac=1)\r\n",
      "\r\n",
      "df_validation = df[df['id_reseller'].isin(val_resellers)].sample(frac=1)\r\n",
      "\r\n",
      "df_train.drop(['date','id_reseller','bill','zone','cluster','weekday'],axis=1,inplace=True)\r\n",
      "df_validation.drop(['date','id_reseller','bill','zone','cluster','weekday'],axis=1,inplace=True)\r\n",
      "\r\n",
      "\r\n",
      "write_dataframe_to_csv_on_s3(df_validation, bucket, 'validation/validation.csv')\r\n",
      "write_dataframe_to_csv_on_s3(df_train, bucket, 'train/train.csv')\r\n",
      "\r\n",
      "#####\r\n",
      "# Preprocessing Pipeline\r\n",
      "#####\r\n",
      "\r\n",
      "df_r = awswrangler.athena.read( \"implementationdb\", \"select * from reseller\" )\r\n",
      "df = awswrangler.athena.read( \"implementationdb\", \"select * from billing\" )\r\n",
      "df['date'] = pd.to_datetime(df['date'])\r\n",
      "\r\n",
      "max_date = df['date'].max()\r\n",
      "min_date = max_date - pd.Timedelta(days=30)\r\n",
      "df = df[(df['date'] > min_date)]\r\n",
      "\r\n",
      "def completeItem(dfItem,max_date,min_date):\r\n",
      "    r = pd.date_range(start=min_date, end=max_date)\r\n",
      "    dfItemNew = dfItem.set_index('date').reindex(r).fillna(0.0).rename_axis('date').reset_index()\r\n",
      "    dfItemNew['id_reseller'] = dfItem['id_reseller'].max()\r\n",
      "    return dfItemNew\r\n",
      "\r\n",
      "dfCompletedList = []\r\n",
      "for nid,item in df.groupby('id_reseller'):\r\n",
      "    dfCompletedList.append(completeItem(item,max_date,min_date))\r\n",
      "dfCompleted = pd.concat(dfCompletedList).copy()\r\n",
      "\r\n",
      "\r\n",
      "df = dfCompleted\r\n",
      "del dfCompleted\r\n",
      "del dfCompletedList\r\n",
      "\r\n",
      "def complete_info(group):\r\n",
      "    weekday = (max_date + pd.Timedelta(days=1)).weekday_name\r\n",
      "    mean_last_30 = group['bill'].replace(0,np.nan).mean()\r\n",
      "    std_last_30 = group['bill'].replace(0,np.nan).std()\r\n",
      "    date_last_bill = group[group['bill'] != 0]['date'].max()\r\n",
      "    days_without_purchase = (max_date + pd.Timedelta(days=1) - date_last_bill).days\r\n",
      "    \r\n",
      "    mean_last_7 = group[(group['date'] >= max_date - pd.Timedelta(days=6))]['bill'].replace(0,np.nan).mean()\r\n",
      "    last_bill = group[group['bill'] > 0].sort_values('date',ascending=False).head(1)['bill'].values[0]\r\n",
      "    return {'weekday':weekday,'mean-last-30':mean_last_30,\r\n",
      "           'std-last-30':std_last_30,'mean-last-7':mean_last_7,'last_bill':last_bill, \r\n",
      "           'id_reseller':group['id_reseller'].max(), 'days_without_purchase':days_without_purchase}\r\n",
      "\r\n",
      "\r\n",
      "features = []\r\n",
      "for index,group in df.groupby('id_reseller'):\r\n",
      "    features.append(complete_info(group))\r\n",
      "    \r\n",
      "df_features = pd.DataFrame(features)\r\n",
      "\r\n",
      "df_features = df_features.merge(df_r,how='inner',on='id_reseller')\r\n",
      "\r\n",
      "pipe_list = [le_cluster,ohe_cluster,le_zone,ohe_zone,le_weekday,ohe_weekday]\r\n",
      "\r\n",
      "df_cluster = pd.DataFrame(\r\n",
      "    pipe_list[1].transform(pipe_list[0].transform(df_features['cluster']).reshape(-1, 1)).todense()\r\n",
      ")\r\n",
      "df_cluster = df_cluster.add_prefix('cluster_')\r\n",
      "df_zone = pd.DataFrame(\r\n",
      "    pipe_list[3].transform(pipe_list[2].transform(df_features['zone']).reshape(-1, 1)).todense()\r\n",
      ")\r\n",
      "df_zone = df_zone.add_prefix('zone_')\r\n",
      "df_weekday = pd.DataFrame(\r\n",
      "    pipe_list[5].transform(pipe_list[4].transform(df_features['weekday']).reshape(-1, 1)).todense()\r\n",
      ")\r\n",
      "df_weekday = df_weekday.add_prefix('weekday_')\r\n",
      "\r\n",
      "df_to_predict = pd.concat([df_features,df_cluster,df_zone,df_weekday],axis=1)\r\n",
      "\r\n",
      "df_to_predict_feats = df_to_predict[['mean-last-30', 'mean-last-7', 'std-last-30',\r\n",
      "       'days_without_purchase', 'last_bill', 'cluster_0', 'cluster_1',\r\n",
      "       'cluster_2', 'cluster_3', 'cluster_4', 'zone_0', 'zone_1', 'zone_2',\r\n",
      "       'zone_3', 'zone_4', 'zone_5', 'zone_6', 'zone_7', 'zone_8', 'zone_9',\r\n",
      "       'zone_10', 'zone_11', 'zone_12', 'zone_13', 'zone_14', 'zone_15',\r\n",
      "       'zone_16', 'zone_17', 'zone_18', 'zone_19', 'zone_20', 'zone_21',\r\n",
      "       'zone_22', 'zone_23', 'zone_24', 'zone_25', 'zone_26', 'zone_27',\r\n",
      "       'zone_28', 'zone_29', 'zone_30', 'zone_31', 'zone_32', 'zone_33',\r\n",
      "       'zone_34', 'zone_35', 'zone_36', 'zone_37', 'zone_38', 'zone_39',\r\n",
      "       'zone_40', 'zone_41', 'zone_42', 'zone_43', 'zone_44', 'zone_45',\r\n",
      "       'zone_46', 'zone_47', 'zone_48', 'zone_49', 'zone_50', 'zone_51',\r\n",
      "       'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4',\r\n",
      "       'weekday_5', 'weekday_6']]\r\n",
      "       \r\n",
      "write_dataframe_to_csv_on_s3(df_to_predict_feats,bucket,'to_predict.csv)\r\n",
      "write_dataframe_to_csv_on_s3(df_to_predict[['id_reseller']],bucket,'id_reseller_to_predict.csv)\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat etlandpipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Orchestration with Lambda, SageMaker, CloudWatch and Step Functions\n",
    "\n",
    "\n",
    "Now it's time to create all the necesary steps to schedule the training, deployment and inference with the model. \n",
    "For this, we are going to use an architecture similar to the <a href='https://github.com/aws-samples/serverless-sagemaker-orchestration'>serverless sagemaker orchestration</a> but adapted to our specific problem.\n",
    "\n",
    "First we need to create lambda functions capables of:\n",
    "\n",
    "    1.training the model\n",
    "    2.awaiting for training\n",
    "    3.deploying the model\n",
    "    4.awaiting for deploy\n",
    "    5.predict, save predictions and delete endpoint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create a role with SageMaker and S3 access\n",
    "\n",
    "To execute this lambdas we are going to need a role SageMaker and S3 permissions.\n",
    "\n",
    "Go to the AWS console and create a role with AmazonS3FullAccess and AmazonSageMakerFullAccess policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train Model Lambda\n",
    "\n",
    "    1. Go to the AWS Console and under Services, select Lambda\n",
    "    2. Go to the Functions Pane and select Create Function\n",
    "    3. Author from scratch\n",
    "    4. Name it lambdaModelTrain, choose runtime Python 3.6 and as executing role, select the role you created in the previous step\n",
    "\n",
    "\n",
    "This lambda function doesn't need to receive any parameters, but it should return the resulting hyperparameter tunning optimization job name, that we will use in the next lambda function to check status, the container it used and that the status is now In Progress...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\r\n",
      "import boto3\r\n",
      "import copy\r\n",
      "from time import gmtime, strftime\r\n",
      "\r\n",
      "\r\n",
      "region = boto3.Session().region_name    \r\n",
      "smclient = boto3.Session().client('sagemaker')\r\n",
      "role = 'arn:aws:iam::452432741922:role/service-role/AmazonSageMaker-ExecutionRole-20181022T121720'\r\n",
      "\r\n",
      "\r\n",
      "bucket_path='s3://blackb-mggaska-implementation'\r\n",
      "prefix = \"invoice-forecast\"\r\n",
      "\r\n",
      "container = '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest'\r\n",
      "\r\n",
      "\r\n",
      "def lambda_handler(event, context):\r\n",
      "        \r\n",
      "    tuning_job_config = {\r\n",
      "    \"ParameterRanges\": {\r\n",
      "      \"CategoricalParameterRanges\": [],\r\n",
      "      \"ContinuousParameterRanges\": [\r\n",
      "        {\r\n",
      "          \"MaxValue\": \"1\",\r\n",
      "          \"MinValue\": \"0\",\r\n",
      "          \"Name\": \"eta\"\r\n",
      "        },\r\n",
      "        {\r\n",
      "          \"MaxValue\": \"2\",\r\n",
      "          \"MinValue\": \"0\",\r\n",
      "          \"Name\": \"alpha\"\r\n",
      "        },\r\n",
      "        {\r\n",
      "          \"MaxValue\": \"10\",\r\n",
      "          \"MinValue\": \"1\",\r\n",
      "          \"Name\": \"min_child_weight\"\r\n",
      "        }\r\n",
      "      ],\r\n",
      "      \"IntegerParameterRanges\": [\r\n",
      "        {\r\n",
      "          \"MaxValue\": \"20\",\r\n",
      "          \"MinValue\": \"10\",\r\n",
      "          \"Name\": \"max_depth\"\r\n",
      "        }\r\n",
      "      ]\r\n",
      "    },\r\n",
      "    \"ResourceLimits\": {\r\n",
      "      \"MaxNumberOfTrainingJobs\": 5,\r\n",
      "      \"MaxParallelTrainingJobs\": 5\r\n",
      "    },\r\n",
      "    \"Strategy\": \"Bayesian\",\r\n",
      "    \"HyperParameterTuningJobObjective\": {\r\n",
      "      \"MetricName\": \"validation:mae\",\r\n",
      "      \"Type\": \"Minimize\"\r\n",
      "    }\r\n",
      "    }\r\n",
      "  \r\n",
      "    training_job_definition = \\\r\n",
      "      { \r\n",
      "        \"AlgorithmSpecification\": {\r\n",
      "            \"TrainingImage\": container,\r\n",
      "            \"TrainingInputMode\": \"File\"\r\n",
      "        },\r\n",
      "        \"RoleArn\": role,\r\n",
      "        \"OutputDataConfig\": {\r\n",
      "            \"S3OutputPath\": bucket_path + \"/\"+ prefix + \"/xgboost\"\r\n",
      "        },\r\n",
      "        \"ResourceConfig\": {\r\n",
      "            \"InstanceCount\": 2,   \r\n",
      "            \"InstanceType\": \"ml.m4.xlarge\",\r\n",
      "            \"VolumeSizeInGB\": 5\r\n",
      "        },\r\n",
      "        \"StaticHyperParameters\": {\r\n",
      "            \"objective\": \"reg:linear\",\r\n",
      "            \"num_round\": \"100\",\r\n",
      "            \"subsample\":\"0.7\",\r\n",
      "            \"eval_metric\":\"mae\"\r\n",
      "        },\r\n",
      "        \"StoppingCondition\": {\r\n",
      "            \"MaxRuntimeInSeconds\": 86400\r\n",
      "        },\r\n",
      "        \"InputDataConfig\": [\r\n",
      "            {\r\n",
      "                \"ChannelName\": \"train\",\r\n",
      "                \"DataSource\": {\r\n",
      "                    \"S3DataSource\": {\r\n",
      "                        \"S3DataType\": \"S3Prefix\",\r\n",
      "                        \"S3Uri\": bucket_path + '/train/',\r\n",
      "                        \"S3DataDistributionType\": \"FullyReplicated\" \r\n",
      "                    }\r\n",
      "                },\r\n",
      "                \"ContentType\": \"text/csv\",\r\n",
      "                \"CompressionType\": \"None\"\r\n",
      "            },\r\n",
      "            {\r\n",
      "                \"ChannelName\": \"validation\",\r\n",
      "                \"DataSource\": {\r\n",
      "                    \"S3DataSource\": {\r\n",
      "                        \"S3DataType\": \"S3Prefix\",\r\n",
      "                        \"S3Uri\": bucket_path + '/validation/',\r\n",
      "                        \"S3DataDistributionType\": \"FullyReplicated\"\r\n",
      "                    }\r\n",
      "                },\r\n",
      "                \"ContentType\": \"text/csv\",\r\n",
      "                \"CompressionType\": \"None\"\r\n",
      "            }\r\n",
      "            ]\r\n",
      "        }\r\n",
      "    \r\n",
      "    tuning_job_name = prefix + strftime(\"%Y%m%d%H%M%S\", gmtime())\r\n",
      "    \r\n",
      "    event[\"container\"] = container\r\n",
      "    event[\"stage\"] = \"Training\"\r\n",
      "    event[\"status\"] = \"InProgress\"\r\n",
      "    event['name'] = tuning_job_name\r\n",
      "    \r\n",
      "    smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\r\n",
      "                                           HyperParameterTuningJobConfig = tuning_job_config,\r\n",
      "                                           TrainingJobDefinition = training_job_definition)\r\n",
      "\r\n",
      "    # output\r\n",
      "    return event"
     ]
    }
   ],
   "source": [
    "! cat lambdafunctions/lambdaModelTrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the lambda function\n",
    "\n",
    "Create a test event with no parameters save and test. In your AWS console under SageMaker>Hyperparamter tunning jobs you should see the HPO runnning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Await Train Model Lambda\n",
    "\n",
    "    1. Go to the AWS Console and under Services, select Lambda\n",
    "    2. Go to the Functions Pane and select Create Function\n",
    "    3. Author from scratch\n",
    "    4. Name it lambdaModelAwait, choose runtime Python 3.6 and as executing role, select the role you created in the previous step\n",
    "    \n",
    "    \n",
    "This lambda function, now receives the output of the previous step and allows us to check if the process is done or not. If it's done, it returns the name of the best training job.\n",
    "\n",
    "Previous response (we can use it in the test event):\n",
    "        \n",
    "        {\n",
    "          \"container\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n",
    "          \"stage\": \"Training\",\n",
    "          \"status\": \"InProgress\",\n",
    "          \"name\": \"invoice-forecast20190702190151\"\n",
    "        }\n",
    "        \n",
    "In the code editor paste the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import boto3\r\n",
      "import os\r\n",
      "\r\n",
      "sagemaker = boto3.client('sagemaker')\r\n",
      "\r\n",
      "\r\n",
      "def lambda_handler(event, context):\r\n",
      "    stage = event['stage']\r\n",
      "    if stage == 'Training':\r\n",
      "        name = event['name']\r\n",
      "        training_details = describe_training_job(name)\r\n",
      "        print(training_details)\r\n",
      "        status = training_details['HyperParameterTuningJobStatus']\r\n",
      "        if status == 'Completed':\r\n",
      "            s3_output_path = training_details['TrainingJobDefinition']['OutputDataConfig']['S3OutputPath']\r\n",
      "            model_data_url = os.path.join(s3_output_path, training_details['BestTrainingJob']['TrainingJobName'], 'output/model.tar.gz')\r\n",
      "            event['message'] = 'HPO tunning job \"{}\" complete. Model data uploaded to \"{}\"'.format(name, model_data_url)\r\n",
      "            event['model_data_url'] = model_data_url\r\n",
      "            event['best_training_job'] = training_details['BestTrainingJob']['TrainingJobName']\r\n",
      "        elif status == 'Failed':\r\n",
      "            failure_reason = training_details['FailureReason']\r\n",
      "            event['message'] = 'Training job failed. {}'.format(failure_reason)\r\n",
      "    elif stage == 'Deployment':\r\n",
      "        name = 'demobb-invoice-prediction'\r\n",
      "        endpoint_details = describe_endpoint(name)\r\n",
      "        status = endpoint_details['EndpointStatus']\r\n",
      "        if status == 'InService':\r\n",
      "            event['message'] = 'Deployment completed for endpoint \"{}\".'.format(name)\r\n",
      "        elif status == 'Failed':\r\n",
      "            failure_reason = endpoint_details['FailureReason']\r\n",
      "            event['message'] = 'Deployment failed for endpoint \"{}\". {}'.format(name, failure_reason)\r\n",
      "        elif status == 'RollingBack':\r\n",
      "            event['message'] = 'Deployment failed for endpoint \"{}\", rolling back to previously deployed version.'.format(name)\r\n",
      "    event['status'] = status\r\n",
      "    return event\r\n",
      "\r\n",
      "\r\n",
      "def describe_training_job(name):\r\n",
      "    \"\"\" Describe SageMaker training job identified by input name.\r\n",
      "    Args:\r\n",
      "        name (string): Name of SageMaker training job to describe.\r\n",
      "    Returns:\r\n",
      "        (dict)\r\n",
      "        Dictionary containing metadata and details about the status of the training job.\r\n",
      "    \"\"\"\r\n",
      "    try:\r\n",
      "        response = sagemaker.describe_hyper_parameter_tuning_job(\r\n",
      "            HyperParameterTuningJobName=name\r\n",
      "        )\r\n",
      "    except Exception as e:\r\n",
      "        print(e)\r\n",
      "        print('Unable to describe hyperparameter tunning job.')\r\n",
      "        raise(e)\r\n",
      "    return response\r\n",
      "    \r\n",
      "def describe_endpoint(name):\r\n",
      "    \"\"\" Describe SageMaker endpoint identified by input name.\r\n",
      "    Args:\r\n",
      "        name (string): Name of SageMaker endpoint to describe.\r\n",
      "    Returns:\r\n",
      "        (dict)\r\n",
      "        Dictionary containing metadata and details about the status of the endpoint.\r\n",
      "    \"\"\"\r\n",
      "    try:\r\n",
      "        response = sagemaker.describe_endpoint(\r\n",
      "            EndpointName=name\r\n",
      "        )\r\n",
      "    except Exception as e:\r\n",
      "        print(e)\r\n",
      "        print('Unable to describe endpoint.')\r\n",
      "        raise(e)\r\n",
      "    return response\r\n"
     ]
    }
   ],
   "source": [
    "! cat lambdafunctions/lambdaModelAwait.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will see a response like\n",
    "\n",
    "    Response:\n",
    "    {\n",
    "      \"container\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n",
    "      \"stage\": \"Training\",\n",
    "      \"status\": \"InProgress\",\n",
    "      \"name\": \"invoice-forecast20190702190151\"\n",
    "    }\n",
    "\n",
    "Once the training is completed, the state is going to change and you'll see the new status and the name of the best training job.\n",
    "\n",
    "    {\n",
    "      \"container\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n",
    "      \"stage\": \"Training\",\n",
    "      \"status\": \"Completed\",\n",
    "      \"name\": \"invoice-forecast20190702190151\",\n",
    "      \"message\": \"HPO tunning job \\\"invoice-forecast20190702190151\\\" complete. Model data uploaded to \\\"s3://blackb-mggaska-implementation/invoice-forecast/xgboost/invoice-forecast20190702190151-005-dde9844e/output/model.tar.gz\\\"\",\n",
    "      \"model_data_url\": \"s3://blackb-mggaska-implementation/invoice-forecast/xgboost/invoice-forecast20190702190151-005-dde9844e/output/model.tar.gz\",\n",
    "      \"best_training_job\": \"invoice-forecast20190702190151-005-dde9844e\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Deploy Model In SageMaker Lambda\n",
    "\n",
    "In this lambda function, we are going to need to use the best training job from the previous step to deploy a predictor. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import boto3\r\n",
      "import os\r\n",
      "\r\n",
      "sagemaker = boto3.client('sagemaker')\r\n",
      "EXECUTION_ROLE = 'arn:aws:iam::452432741922:role/service-role/AmazonSageMaker-ExecutionRole-20181022T121720'\r\n",
      "INSTANCE_TYPE = 'ml.m4.xlarge'\r\n",
      "container = '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest'\r\n",
      "\r\n",
      "def lambda_handler(event, context):\r\n",
      "    best_training_job = event['best_training_job']\r\n",
      "    endpoint = 'demobb-invoice-prediction'\r\n",
      "    model_data_url = event['model_data_url']\r\n",
      "    print('Creating model resource from training artifact...')\r\n",
      "    create_model(best_training_job, container, model_data_url)\r\n",
      "    print('Creating endpoint configuration...')\r\n",
      "    create_endpoint_config(best_training_job)\r\n",
      "    print('There is no existing endpoint for this model. Creating new model endpoint...')\r\n",
      "    create_endpoint(endpoint, best_training_job)\r\n",
      "    event['stage'] = 'Deployment'\r\n",
      "    event['status'] = 'Creating'\r\n",
      "    event['message'] = 'Started deploying model \"{}\" to endpoint \"{}\"'.format(best_training_job, endpoint)\r\n",
      "    return event\r\n",
      "\r\n",
      "def create_model(name, container, model_data_url):\r\n",
      "    \"\"\" Create SageMaker model.\r\n",
      "    Args:\r\n",
      "        name (string): Name to label model with\r\n",
      "        container (string): Registry path of the Docker image that contains the model algorithm\r\n",
      "        model_data_url (string): URL of the model artifacts created during training to download to container\r\n",
      "    Returns:\r\n",
      "        (None)\r\n",
      "    \"\"\"\r\n",
      "    try:\r\n",
      "        sagemaker.create_model(\r\n",
      "            ModelName=name,\r\n",
      "            PrimaryContainer={\r\n",
      "                'Image': container,\r\n",
      "                'ModelDataUrl': model_data_url\r\n",
      "            },\r\n",
      "            ExecutionRoleArn=EXECUTION_ROLE\r\n",
      "        )\r\n",
      "    except Exception as e:\r\n",
      "        print(e)\r\n",
      "        print('Unable to create model.')\r\n",
      "        raise(e)\r\n",
      "    \r\n",
      "def create_endpoint_config(name):\r\n",
      "    \"\"\" Create SageMaker endpoint configuration. \r\n",
      "    Args:\r\n",
      "        name (string): Name to label endpoint configuration with.\r\n",
      "    Returns:\r\n",
      "        (None)\r\n",
      "    \"\"\"\r\n",
      "    try:\r\n",
      "        sagemaker.create_endpoint_config(\r\n",
      "            EndpointConfigName=name,\r\n",
      "            ProductionVariants=[\r\n",
      "                {\r\n",
      "                    'VariantName': 'prod',\r\n",
      "                    'ModelName': name,\r\n",
      "                    'InitialInstanceCount': 1,\r\n",
      "                    'InstanceType': INSTANCE_TYPE\r\n",
      "                }\r\n",
      "            ]\r\n",
      "        )\r\n",
      "    except Exception as e:\r\n",
      "        print(e)\r\n",
      "        print('Unable to create endpoint configuration.')\r\n",
      "        raise(e)\r\n",
      "def create_endpoint(endpoint_name, config_name):\r\n",
      "    \"\"\" Create SageMaker endpoint with input endpoint configuration.\r\n",
      "    Args:\r\n",
      "        endpoint_name (string): Name of endpoint to create.\r\n",
      "        config_name (string): Name of endpoint configuration to create endpoint with.\r\n",
      "    Returns:\r\n",
      "        (None)\r\n",
      "    \"\"\"\r\n",
      "    try:\r\n",
      "        sagemaker.create_endpoint(\r\n",
      "            EndpointName=endpoint_name,\r\n",
      "            EndpointConfigName=config_name\r\n",
      "        )\r\n",
      "    except Exception as e:\r\n",
      "        print(e)\r\n",
      "        print('Unable to create endpoint.')\r\n",
      "        raise(e)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat lambdafunctions/lambdaModelDeploy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On your SageMaker console you should see an endpoint with status creating. Once you test the output it should look like this:\n",
    "\n",
    "    {\n",
    "      \"container\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n",
    "      \"stage\": \"Deployment\",\n",
    "      \"status\": \"Creating\",\n",
    "      \"name\": \"invoice-forecast20190702190151\",\n",
    "      \"message\": \"Started deploying model \\\"invoice-forecast20190702190151-005-dde9844e\\\" to endpoint \\\"demobb-invoice-prediction\\\"\",\n",
    "      \"model_data_url\": \"s3://blackb-mggaska-implementation/invoice-forecast/xgboost/invoice-forecast20190702190151-005-dde9844e/output/model.tar.gz\",\n",
    "      \"best_training_job\": \"invoice-forecast20190702190151-005-dde9844e\"\n",
    "    }\n",
    "\n",
    "### This is a good chance to test the Await function this time with Deployment stage.\n",
    "\n",
    "If you create a new test event on the lambdaAwaitModel you should see a response like this:\n",
    "\n",
    "    Response:\n",
    "    {\n",
    "      \"container\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n",
    "      \"stage\": \"Deployment\",\n",
    "      \"status\": \"Creating\",\n",
    "      \"name\": \"invoice-forecast20190702190151\",\n",
    "      \"message\": \"Started deploying model \\\"invoice-forecast20190702190151-005-dde9844e\\\" to endpoint \\\"demobb-invoice-prediction\\\"\",\n",
    "      \"model_data_url\": \"s3://blackb-mggaska-implementation/invoice-forecast/xgboost/invoice-forecast20190702190151-005-dde9844e/output/model.tar.gz\",\n",
    "      \"best_training_job\": \"invoice-forecast20190702190151-005-dde9844e\"\n",
    "    }\n",
    "\n",
    "After the model is in service you should see something like this:\n",
    "\n",
    "    {\n",
    "      \"container\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n",
    "      \"stage\": \"Deployment\",\n",
    "      \"status\": \"InService\",\n",
    "      \"name\": \"invoice-forecast20190702190151\",\n",
    "      \"message\": \"Deployment completed for endpoint \\\"demobb-invoice-prediction\\\".\",\n",
    "      \"model_data_url\": \"s3://blackb-mggaska-implementation/invoice-forecast/xgboost/invoice-forecast20190702190151-005-dde9844e/output/model.tar.gz\",\n",
    "      \"best_training_job\": \"invoice-forecast20190702190151-005-dde9844e\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Use model to predict\n",
    "\n",
    "Now we are going to use the deployed model to predict. This last lambda function doesn't take any parameters. But in this case we need to touch the default parameters of the lambda to configure Max Memory in 1024 MB and Timeout in 15 Mins. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import io\r\n",
      "import boto3\r\n",
      "import json\r\n",
      "import csv\r\n",
      "from io import StringIO\r\n",
      "\r\n",
      "# grab static variables\r\n",
      "ENDPOINT_NAME = 'demobb-invoice-prediction'\r\n",
      "runtime= boto3.client('runtime.sagemaker')\r\n",
      "bucket = 'blackb-mggaska-implementation'\r\n",
      "s3 = boto3.client('s3')\r\n",
      "bucket ='blackb-mggaska-implementation'\r\n",
      "key = 'to_predict.csv'\r\n",
      "def lambda_handler(event, context):\r\n",
      "    response = s3.get_object(Bucket=bucket, Key=key)\r\n",
      "    content = response['Body'].read().decode('utf-8')\r\n",
      "    results = []\r\n",
      "    for line in  content.splitlines():\r\n",
      "        response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\r\n",
      "                                               ContentType='text/csv',\r\n",
      "                                               Body=line)\r\n",
      "        result = json.loads(response['Body'].read().decode())\r\n",
      "        results.append(result)\r\n",
      "        i = 0\r\n",
      "    multiLine = \"\"\r\n",
      "    for item in results:\r\n",
      "        if (i > 0):\r\n",
      "            multiLine = multiLine + '\\n'\r\n",
      "        multiLine = multiLine + str(item)\r\n",
      "        i+=1\r\n",
      "    \r\n",
      "    file_name = \"predictions.csv\" \r\n",
      "    s3_resource = boto3.resource('s3')\r\n",
      "    s3_resource.Object(bucket, file_name).put(Body=multiLine)\r\n",
      "\r\n",
      "\r\n",
      "    event['status'] = 'Processed records ' + str(len(results))\r\n",
      "    return event"
     ]
    }
   ],
   "source": [
    "! cat lambdafunctions/lambdaModelPredict.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
