{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation with Step Functions\n",
    "\n",
    "Now that we have a satisfying machine learning model, we want to implement this as part of a process that runs every day at 3AM in the morning based on the latest transactional information that we have dumped on S3 to create forecasts for each reseller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload data to S3\n",
    "\n",
    "First you need to create a bucket for this experiment. Upload the data from the following public location to your own S3 bucket. To facilitate the work of the crawler use two different prefixs (folders): one for the billing information and one for reseller. \n",
    "\n",
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your bucket name\n",
    "your_bucket = 'blackb-mggaska-implementation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-02 16:35:59--  https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/billing_sm.csv\n",
      "Resolving ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)... 52.216.20.203\n",
      "Connecting to ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)|52.216.20.203|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15803443 (15M) [binary/octet-stream]\n",
      "Saving to: ‘billing_sm.csv.1’\n",
      "\n",
      "billing_sm.csv.1    100%[===================>]  15.07M  99.2MB/s    in 0.2s    \n",
      "\n",
      "2019-07-02 16:35:59 (99.2 MB/s) - ‘billing_sm.csv.1’ saved [15803443/15803443]\n",
      "\n",
      "--2019-07-02 16:35:59--  https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/reseller_sm.csv\n",
      "Resolving ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)... 52.216.20.203\n",
      "Connecting to ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)|52.216.20.203|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 210111 (205K) [binary/octet-stream]\n",
      "Saving to: ‘reseller_sm.csv.1’\n",
      "\n",
      "reseller_sm.csv.1   100%[===================>] 205.19K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2019-07-02 16:35:59 (32.0 MB/s) - ‘reseller_sm.csv.1’ saved [210111/210111]\n",
      "\n",
      "--2019-07-02 16:35:59--  https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/awswrangler-0.0b2-py3.6.egg\n",
      "Resolving ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)... 52.216.20.203\n",
      "Connecting to ml-lab-mggaska.s3.amazonaws.com (ml-lab-mggaska.s3.amazonaws.com)|52.216.20.203|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33511 (33K) [binary/octet-stream]\n",
      "Saving to: ‘awswrangler-0.0b2-py3.6.egg’\n",
      "\n",
      "awswrangler-0.0b2-p 100%[===================>]  32.73K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2019-07-02 16:35:59 (49.1 MB/s) - ‘awswrangler-0.0b2-py3.6.egg’ saved [33511/33511]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/billing_sm.csv\n",
    "!wget https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/reseller_sm.csv\n",
    "!wget https://ml-lab-mggaska.s3.amazonaws.com/sales-forecast/awswrangler-0.0b2-py3.6.egg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we upload to an S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, os\n",
    "boto3.Session().resource('s3').Bucket(your_bucket).Object(os.path.join('billing', 'billing_sm.csv')).upload_file('billing_sm.csv')\n",
    "boto3.Session().resource('s3').Bucket(your_bucket).Object(os.path.join('reseller', 'reseller_sm.csv')).upload_file('reseller_sm.csv')\n",
    "boto3.Session().resource('s3').Bucket(your_bucket).Object(os.path.join('python', 'awswrangler-0.0b2-py3.6.egg')).upload_file('awswrangler-0.0b2-py3.6.egg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Crawler\n",
    "\n",
    "To use this csv information in the context of a Glue ETL, first we have to create a Glue crawler pointing to the location of each file. The crawler will try to figure out the data types of each column. The safest way to do this process is to create one crawler for each table pointing to a different location.\n",
    "\n",
    "1. Go to the AWS Console.\n",
    "2. Select under Services AWS Glue.\n",
    "3. Under crawlers Add Crawler and two crawlers: create one pointing to each S3 location (one to billing and one to reseller)\n",
    "\n",
    "    3.1 Name: Billing, Data Store, Specific Path in my Account, Navigate to your bucket and your folder Billing, create an IAM role billing-crawler-role, add database implementationdb, Next, Finish\n",
    "    \n",
    "    3.2 After the crawler is created select Run it now.\n",
    "    \n",
    "    3.3 Name: Reseller, Data Store, Specific Path in my Account, Navigate to your bucket and your folder Reseller, create an IAM role reseller-crawler-role, select database implementationdb, Next, Finish\n",
    "    \n",
    "    3.4 After the crawler is created select Run it now.\n",
    "\n",
    "After both crawlers run you should see one table is been adeed for each. You can use Athena to inspect the tables and double check the data is been added properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Glue Job\n",
    "\n",
    "Now we are going to create a GLUE ETL job in python 3.6. In this job we can combine both the ETL from Notebook #2 and the Preprocessing Pipeline from Notebook #4.\n",
    "\n",
    "Note that instead of reading from a csv file we are going to use Athena to read from the resulting tables of the Glue Crawler. \n",
    "\n",
    "Glue is a \"serverless\" service so the processing power assigned to the process is meassured in DPUs. Each DPU is equivalent to 16GB of RAM and 4vCPU. \n",
    "\n",
    "1. Open the AWS Console\n",
    "2. Under Services go to AWS Glue\n",
    "3. Uner Jobs, add new job\n",
    "4. Name: etlandpipeline, Role: Glueadmin, Type Python Shell, Python3, Select A New Script Authored By you,\n",
    "Under Security Configuration...  Select Python library path and browse to the location where you have the egg of the aws wrangler Library, Under Maximum Capacity write 1. Then hit \"Save Job and Edit Script\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
